#!/usr/bin/env node

import { execSync } from 'child_process'
import { performance } from 'perf_hooks'
import chalk from 'chalk'

console.log(chalk.blue('üß™ RepoMed IA - Suite Completa de Testes'))
console.log(chalk.gray('=' .repeat(50)))

const startTime = performance.now()
let totalTests = 0
let passedTests = 0
let failedTests = 0
const results = {}

// Utility function to run command and capture results
function runCommand(command, description, options = {}) {
  console.log(chalk.yellow(`\nüèÉ Executando: ${description}`))
  console.log(chalk.gray(`Comando: ${command}`))
  
  const testStartTime = performance.now()
  
  try {
    const output = execSync(command, {
      encoding: 'utf8',
      stdio: 'pipe',
      cwd: process.cwd(),
      ...options
    })
    
    const duration = ((performance.now() - testStartTime) / 1000).toFixed(2)
    console.log(chalk.green(`‚úÖ ${description} - Conclu√≠do em ${duration}s`))
    
    results[description] = {
      status: 'passed',
      duration,
      output: output.trim()
    }
    
    passedTests++
    return { success: true, output }
    
  } catch (error) {
    const duration = ((performance.now() - testStartTime) / 1000).toFixed(2)
    console.log(chalk.red(`‚ùå ${description} - Falhou em ${duration}s`))
    console.log(chalk.red(`Erro: ${error.message}`))
    
    if (error.stdout) {
      console.log(chalk.gray('STDOUT:'))
      console.log(error.stdout.toString())
    }
    
    if (error.stderr) {
      console.log(chalk.gray('STDERR:'))
      console.log(error.stderr.toString())
    }
    
    results[description] = {
      status: 'failed',
      duration,
      error: error.message,
      stdout: error.stdout?.toString(),
      stderr: error.stderr?.toString()
    }
    
    failedTests++
    return { success: false, error }
  } finally {
    totalTests++
  }
}

// Main test execution
async function runAllTests() {
  console.log(chalk.blue('\nüìã Iniciando execu√ß√£o da suite de testes...'))
  
  // 1. Lint and Format Check
  console.log(chalk.magenta('\nüîç FASE 1: Qualidade de C√≥digo'))
  runCommand('npm run lint', 'Verifica√ß√£o de Lint')
  runCommand('npx prettier --check .', 'Verifica√ß√£o de Formata√ß√£o')
  
  // 2. Unit Tests - API
  console.log(chalk.magenta('\nüß™ FASE 2: Testes Unit√°rios - Backend'))
  runCommand('cd repomed-api && npm test -- --reporter=verbose', 'Testes Unit√°rios API')
  
  // 3. Unit Tests - Web
  console.log(chalk.magenta('\nüß™ FASE 3: Testes Unit√°rios - Frontend'))
  runCommand('cd repomed-web && npm test -- --reporter=verbose', 'Testes Unit√°rios Web')
  
  // 4. Integration Tests
  console.log(chalk.magenta('\nüîó FASE 4: Testes de Integra√ß√£o'))
  runCommand('cd repomed-api && npm test -- --run tests/integration/', 'Testes de Integra√ß√£o API')
  
  // 5. Contract Tests
  console.log(chalk.magenta('\nüìù FASE 5: Testes de Contrato'))
  runCommand('npm run test:contracts', 'Testes de Contratos Zod')
  
  // 6. Build Tests
  console.log(chalk.magenta('\nüèóÔ∏è  FASE 6: Testes de Build'))
  runCommand('npm run build', 'Build Completo')
  
  // 7. Load Tests (if API is running)
  console.log(chalk.magenta('\n‚ö° FASE 7: Testes de Carga'))
  
  // Check if API is running
  try {
    const healthCheck = await fetch('http://localhost:8082/health')
    if (healthCheck.ok) {
      runCommand('npm run test:load', 'Testes de Carga')
    } else {
      console.log(chalk.yellow('‚ö†Ô∏è  API n√£o est√° rodando, pulando testes de carga'))
      results['Testes de Carga'] = { status: 'skipped', reason: 'API n√£o dispon√≠vel' }
      totalTests++
    }
  } catch (error) {
    console.log(chalk.yellow('‚ö†Ô∏è  API n√£o est√° rodando, pulando testes de carga'))
    results['Testes de Carga'] = { status: 'skipped', reason: 'API n√£o dispon√≠vel' }
    totalTests++
  }
  
  // 8. E2E Tests (if services are running)
  console.log(chalk.magenta('\nüé≠ FASE 8: Testes End-to-End'))
  
  try {
    const webCheck = await fetch('http://localhost:3002')
    const apiCheck = await fetch('http://localhost:8082/health')
    
    if (webCheck.ok && apiCheck.ok) {
      runCommand('npm run test:e2e', 'Testes End-to-End')
    } else {
      console.log(chalk.yellow('‚ö†Ô∏è  Servi√ßos n√£o est√£o rodando, pulando testes E2E'))
      results['Testes End-to-End'] = { status: 'skipped', reason: 'Servi√ßos n√£o dispon√≠veis' }
      totalTests++
    }
  } catch (error) {
    console.log(chalk.yellow('‚ö†Ô∏è  Servi√ßos n√£o est√£o rodando, pulando testes E2E'))
    results['Testes End-to-End'] = { status: 'skipped', reason: 'Servi√ßos n√£o dispon√≠veis' }
    totalTests++
  }
  
  // Generate comprehensive report
  generateReport()
}

function generateReport() {
  const totalDuration = ((performance.now() - startTime) / 1000).toFixed(2)
  
  console.log(chalk.blue('\n' + '=' .repeat(60)))
  console.log(chalk.blue('üìä RELAT√ìRIO FINAL DA SUITE DE TESTES'))
  console.log(chalk.blue('=' .repeat(60)))
  
  // Summary statistics
  console.log(chalk.white('\nüìà RESUMO:'))
  console.log(`   Total de testes: ${totalTests}`)
  console.log(chalk.green(`   ‚úÖ Aprovados: ${passedTests}`))
  console.log(chalk.red(`   ‚ùå Reprovados: ${failedTests}`))
  
  const skippedTests = Object.values(results).filter(r => r.status === 'skipped').length
  if (skippedTests > 0) {
    console.log(chalk.yellow(`   ‚è≠Ô∏è  Pulados: ${skippedTests}`))
  }
  
  console.log(`   ‚è±Ô∏è  Tempo total: ${totalDuration}s`)
  console.log(`   üìä Taxa de sucesso: ${((passedTests / totalTests) * 100).toFixed(1)}%`)
  
  // Detailed results
  console.log(chalk.white('\nüìã RESULTADOS DETALHADOS:'))
  
  Object.entries(results).forEach(([testName, result]) => {
    const statusIcon = {
      'passed': '‚úÖ',
      'failed': '‚ùå',
      'skipped': '‚è≠Ô∏è'
    }[result.status]
    
    const statusColor = {
      'passed': chalk.green,
      'failed': chalk.red,
      'skipped': chalk.yellow
    }[result.status]
    
    console.log(`   ${statusIcon} ${statusColor(testName)} ${result.duration ? `(${result.duration}s)` : ''}`)
    
    if (result.reason) {
      console.log(chalk.gray(`      Motivo: ${result.reason}`))
    }
    
    if (result.error) {
      console.log(chalk.red(`      Erro: ${result.error.substring(0, 100)}...`))
    }
  })
  
  // Performance insights
  const durations = Object.values(results)
    .filter(r => r.duration)
    .map(r => parseFloat(r.duration))
    .sort((a, b) => b - a)
  
  if (durations.length > 0) {
    console.log(chalk.white('\n‚ö° PERFORMANCE:'))
    console.log(`   Teste mais lento: ${durations[0]}s`)
    console.log(`   Teste mais r√°pido: ${durations[durations.length - 1]}s`)
    console.log(`   Tempo m√©dio: ${(durations.reduce((a, b) => a + b, 0) / durations.length).toFixed(2)}s`)
  }
  
  // Quality gates
  console.log(chalk.white('\nüéØ CRIT√âRIOS DE QUALIDADE:'))
  
  const qualityCriteria = [
    { name: 'Taxa de sucesso > 80%', passed: (passedTests / totalTests) > 0.8 },
    { name: 'Zero falhas cr√≠ticas', passed: failedTests === 0 },
    { name: 'Tempo total < 10min', passed: parseFloat(totalDuration) < 600 },
    { name: 'Build bem-sucedido', passed: results['Build Completo']?.status === 'passed' }
  ]
  
  let criteriaMet = 0
  qualityCriteria.forEach(criteria => {
    const icon = criteria.passed ? '‚úÖ' : '‚ùå'
    const color = criteria.passed ? chalk.green : chalk.red
    console.log(`   ${icon} ${color(criteria.name)}`)
    if (criteria.passed) criteriaMet++
  })
  
  // Final verdict
  const overallSuccess = criteriaMet === qualityCriteria.length && failedTests === 0
  
  console.log(chalk.blue('\n' + '=' .repeat(60)))
  console.log(`üèÜ RESULTADO GERAL: ${overallSuccess ? chalk.green('‚úÖ APROVADO') : chalk.red('‚ùå REPROVADO')}`)
  console.log(chalk.gray(`   Crit√©rios atendidos: ${criteriaMet}/${qualityCriteria.length}`))
  console.log(chalk.blue('=' .repeat(60)))
  
  // Save report to file
  const reportData = {
    timestamp: new Date().toISOString(),
    summary: {
      total: totalTests,
      passed: passedTests,
      failed: failedTests,
      skipped: skippedTests,
      duration: totalDuration,
      successRate: (passedTests / totalTests) * 100
    },
    results,
    qualityCriteria: qualityCriteria.map(c => ({ ...c, status: c.passed ? 'met' : 'unmet' })),
    overallSuccess
  }
  
  try {
    const fs = await import('fs')
    fs.writeFileSync('./test-results/test-report.json', JSON.stringify(reportData, null, 2))
    console.log(chalk.gray('\nüìÑ Relat√≥rio salvo em: ./test-results/test-report.json'))
  } catch (error) {
    console.log(chalk.yellow('\n‚ö†Ô∏è  N√£o foi poss√≠vel salvar o relat√≥rio em arquivo'))
  }
  
  // Exit with appropriate code
  process.exit(overallSuccess ? 0 : 1)
}

// Handle uncaught errors
process.on('uncaughtException', (error) => {
  console.log(chalk.red('\nüí• Erro n√£o capturado:'))
  console.error(error)
  process.exit(1)
})

process.on('unhandledRejection', (reason) => {
  console.log(chalk.red('\nüí• Promise rejeitada:'))
  console.error(reason)
  process.exit(1)
})

// Execute tests
if (import.meta.url === `file://${process.argv[1]}`) {
  runAllTests().catch(error => {
    console.error(chalk.red('Erro fatal na execu√ß√£o dos testes:'), error)
    process.exit(1)
  })
}